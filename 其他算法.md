## 分类模型和回归模型的区别

回归应该是去拟合一个数值; 而分类是要预测类别, 输出离散的结果. 这应该是直观的区别了. 

## 分类模型可以做回归分析吗？反过来可以吗？ 

[参考](https://www.zhihu.com/question/21329754)

有人说分类模型和回归模型本质上是一样的, 仿佛也就是最后输出的时候套了个不一样的喷口罢了. 

也有人说上述说法损失了数学上的严谨性. 

或者说, 现实在做的时候, 你当然可以说是一样的, 因为你弄一弄, blabla, 也就能够在回归和分类之间反复切换了; 但让你讲道理, 解释为什么能酱紫, 你有信心说服别人吗? 这就是个问题了. 

## 判别模型，生成模型

《统计学习方法》里面曰: 
* 判别模型: 直接学习决策函数或者是P(Y|X). 
* 生成模型: 通过联合概率分布P(X, Y)来求得条件概率分布P(Y|X). 

## KNN（分类与回归） 

分类: 就是看一个点, 离他最近的K个点里面哪个标签最多, 那么这个点就是这个标签. [参考](https://zhuanlan.zhihu.com/p/25994179)

回归: 就是看一个点, 把离他最近的k个样例平均一下, 就得到了这个点的预测值. <https://blog.csdn.net/zrh_CSDN/article/details/80679428>

## kmeans的原理，优缺点以及改进。

KMeans是聚类算法. 

选K个点作为初始的质心, 然后把每一个点归类为离他们最近的质心所表示的那个类. 然后调整质心的位置, 再做一轮质心划分. 一直到质心的位置不再改动为止. 

## k-means的k怎么取

## k-means算法初始点怎么选择？

原则是, K个点尽量远. 比如, 第一个点随机设, 第二个点放远点, 第三个点距离前两个点也都远一点. <https://blog.csdn.net/HUSTLX/article/details/51362267>

## EM算法: 

<https://zhuanlan.zhihu.com/p/40991784>

## em 与 kmeans 的关系

KMeans是EM算法的特例. <https://www.zhihu.com/question/49972233>

## k折交叉验证中k取值多少有什么关系

[参考](https://zhuanlan.zhihu.com/p/24825503)

K-Fold 交叉验证. 就是分k份, 一份测试, 剩下的训练. 

如果K太大, 那么每一次分到的训练集都差不多, 那么这样训练出来的k个模型也就差不多了(具有很大的相关性), 那么variance就会很大. 

根据经验, K取5或者10就可以了. 

## 什么是贝叶斯估计 

## 什么是过拟合? 

## 什么情况下一定会发生过拟合？ 

[参考](https://zhuanlan.zhihu.com/p/26122044)

* 数据有噪声
* 数据太少
* 模型太复杂

## 如何防止过拟合

*增加数据，减少模型复杂度->正则化.*

## 什么是欠拟合?

[参考](https://zhuanlan.zhihu.com/p/29707029)

拟合得不够呗. 

怎么解决: 
* 增加特征
* 非线性模型去拟合
* 调小正则项, 增大模型复杂度等. 

## 在模型的训练迭代中，怎么评估效果。 

这个题目我不太明白要从什么角度解答. 姑妄言之: 

评价一个模型的好坏有好些指标. 什么accuracy, precision, recall, F1, AUC/ROC之类. [参考](https://zhuanlan.zhihu.com/p/43405406)

![avatar](https://raw.githubusercontent.com/songapore/For-PicGo/master/img/%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE.jpg)

取到那个total error, 应该也就是泛化误差最小的地方就可以了. 

## 归一化方式

[参考](https://www.zhihu.com/question/20467170)

顾名思义, 把数据的范围压缩到1附近(比如[0, 1]区间). 有很多种具体的方式. 

归一化应该也算是一种标准化, 但是标准化的范围更大, 可能不只是归到一附近, 而是其他的值的附近. 

## 为什么要做数据归一化？

[参考](https://www.zhihu.com/question/20455227)

有人认为, *数据归一化后，最优解的寻优过程明显会变得平缓，更容易正确的收敛到最优解。*
![avatar](https://pic2.zhimg.com/v2-a0cf11340fc1a026405ffa489e21d6bd_r.jpg?source=1940ef5c)
![avatar](https://picb.zhimg.com/v2-756c8d2c55df7013f9879dc5ca3e87a4_r.jpg?source=1940ef5c)

## 特征选择的方法 

[参考1](https://juejin.im/post/6844903941356912647), [参考2](https://zhuanlan.zhihu.com/p/74198735)

* *Filter(过滤法)：按照发散性或相关性对各个特征进行评分，设定阈值或者待选择特征的个数进行筛选.* 删掉一些不需要的特征. 

* *Wrapper(包装法)：根据目标函数（往往是预测效果评分），每次选择若干特征，或者排除若干特征.* 选一部分特征试着拟合一个模型, 看效果好不好. 注意, 试的模型和正式的模型的算法应该是一样的. 

* *Embedded(嵌入法)：先使用某些机器学习的模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征（类似于Filter，只不过系数是通过训练得来的）* 还能基于L1的系数特性来选取特征. 

## 由数据引申到数据不平衡怎么处理（10W正例，1W负例，牛客上有原题）

[参考](https://zhuanlan.zhihu.com/p/32940093)

* 欠采样: 多的那部分数据, 舍弃掉一部分, 使得它与少的那部分差不多. 
* 过采样: 重复使用一部分少的那个数据, 使得它与多的那部分差不多. 
* 改变阈值: 如果平衡的话, 本该设置分类阈值为0.5; 既然不平衡, 那么这个阈值也得变. 
* 使用一些叼的算法, 比如XGBoost, RF之类, 辅以调节阈值. 
* 使用AUC酱紫的对类别分布不是很敏感的评估标准. 

## 常见分类模型（ svm，决策树，贝叶斯等）的优缺点，适用场景以及如何选型 

## 信息熵公式

信息熵这个概念太泛, 不如探索一下所谓[交叉熵](https://zhuanlan.zhihu.com/p/38241764)之类的东东. 以后再搞. 咕咕咕. 

