## 加权方式

参考: <https://zhuanlan.zhihu.com/p/141690284>, 

* 线性加权
* bagging: 放回取样n轮, 然后训练n个模型, 然后voting来获取最后的结果. 这么做有助于降低variance. 
* boosting: 基于前一个模型再训练一个模型. 有一个特点, 就是总会考虑到前面的模型的失误. 能够降低bias. 
    * adaboost: 前一轮分类错的样本, 在下一轮中有更大的权重, 受到更多关注. 
    * GBDT和XGBoost: 前一轮的模型得到的预测和目标之间的残差, 用新一轮的模型去拟合. 就是, 用一个新的模型, 去弥补前面的模型残差. 然后, XGBoost是在一些细节上优化了的GBDT. 
* stacking: n个基模型在原始数据上训练后, 得到n个分类结果, 然后这n个分类结果作为新一轮的特征, 输入到一个meta classifier里面去, 得到最后的预测输出. 
* cascade: 据称会非常非常准确, 适用于犯错代价高的情形. 简单的原理描述如下: 用训练数据去训练一个分类器, 用这个模型给测试数据样本分类, 有一些分类会被很笃定地认定(比如给出了一个概率值, 高于判断阈值)为某种分类, 那么这样的样本就可以不管了; 然后单独获取那些不能够很笃定判断(比如给出的概率小于预设的阈值)的样本来训练一个新的模型, 直到最后. [参考](https://zhuanlan.zhihu.com/p/115245324) 注意, 参考给出的解释不是很清晰, 有一些问题没有很好解释, 比如训练了多个模型后怎么将他们融合起来; 后续的模型是基于test dataset来训练的吗; 每一次训练的新模型, 是从空白开始训练起还是基于前一个模型进行fine-tuning? 

## adaboost
* 算法原理: [参考](https://zhuanlan.zhihu.com/p/141690284)

## 随机森林

<https://easyai.tech/ai-definition/random-forest/>

* 利用了bagging思想进行集成. 基分类器是CART. 
* 有放回地抽取样本, 进行训练, 得到一棵树. 然后很多这样的树构成森林. 每一棵树在构建的时候, 只选取部分的特征来进行分裂. 
* 优点: 
    * 无需降维; 
    * 能够判断特征的重要程度; 
    * 特征之间的影响也能判断; 
    * 不易过拟合; 
    * 训练快, 能并行; 
    * 实现较为简单; 
    * *对于不平衡的数据集来说，它可以平衡误差;* 就算有很多的特征遗失, 也能维持准确度.  
* 缺点:     
    * 某些噪声较大的分类/回归问题上, 还是容易过拟合; 
    * *对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的.*

## GBDT

[参考1](https://puluwen.github.io/2019/01/GBDT-introduction/), [参考2](https://www.jianshu.com/p/856ea8201f63)

* 最基本的含义: *利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中的残差的近似值，拟合一个回归树.* 最基本的思想就是, 用后来的模型去拟合(或者形象一点, 弥补)前面模型预测结果与真实值之间的残差. 
* 然后呢, 这里面的梯度指什么呢? 是用某个梯度的值来作为这个所谓的残差.  
* 优点: 
    * 防止过拟合; 
    * 非线性变化多, 表达能力强, 无需太复杂的特征工程和变换; 
    * 可解释性强; 
* 缺点: 
    * 不利于串行化. 

## RF, GBDT, XGBoost的优缺点

[参考1](https://www.jianshu.com/p/7c621813728c), [参考2](https://zhuanlan.zhihu.com/p/72247243)

* RF Vs. GBDT
    * 随机森林可以用回归树和分类树, 但是GBDT只能用回归树. 
    * 前者可串行, 后者不行. 
    * 前者最后的结果是投票选出来的; 后者是将所有的结果加权累加得到的. 
    * 前者对异常值比较不敏感; 后者敏感. 
    * *随机森林对训练集一视同仁，GBDT 是基于权值的弱分类器的集成。* 我个人认为, 前半句指的是随机森林对每一个基分类器没有区别对待. 
    * 前者减少方差variance, 后者减少偏差bias. 

* GBDT Vs. XGBoost


