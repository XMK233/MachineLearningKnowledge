## 加权方式

参考: <https://zhuanlan.zhihu.com/p/141690284>, 

* 线性加权
* bagging: 放回取样n轮, 然后训练n个模型, 然后voting来获取最后的结果. 这么做有助于降低variance. 
* boosting: 基于前一个模型再训练一个模型. 有一个特点, 就是总会考虑到前面的模型的失误. 能够降低bias. 
    * adaboost: 前一轮分类错的样本, 在下一轮中有更大的权重, 受到更多关注. 
    * GBDT和XGBoost: 前一轮的模型得到的预测和目标之间的残差, 用新一轮的模型去拟合. 就是, 用一个新的模型, 去弥补前面的模型残差. 然后, XGBoost是在一些细节上优化了的GBDT. 
* stacking: n个基模型在原始数据上训练后, 得到n个分类结果, 然后这n个分类结果作为新一轮的特征, 输入到一个meta classifier里面去, 得到最后的预测输出. 
* cascade: 据称会非常非常准确, 适用于犯错代价高的情形. 简单的原理描述如下: 用训练数据去训练一个分类器, 用这个模型给测试数据样本分类, 有一些分类会被很笃定地认定(比如给出了一个概率值, 高于判断阈值)为某种分类, 那么这样的样本就可以不管了; 然后单独获取那些不能够很笃定判断(比如给出的概率小于预设的阈值)的样本来训练一个新的模型, 直到最后. [参考](https://zhuanlan.zhihu.com/p/115245324) 注意, 参考给出的解释不是很清晰, 有一些问题没有很好解释, 比如训练了多个模型后怎么将他们融合起来; 后续的模型是基于test dataset来训练的吗; 每一次训练的新模型, 是从空白开始训练起还是基于前一个模型进行fine-tuning? 

## adaboost

[参考1](https://zhuanlan.zhihu.com/p/141690284), [参考2](https://zhuanlan.zhihu.com/p/34534004)

前一轮的训练, 预测错了的样本, 在后续的训练中会得到更多的关注. 然后再将每一轮得到的模型加权线性组合一下, 得到最后的分类器. 

## 随机森林

<https://easyai.tech/ai-definition/random-forest/>

* 利用了bagging思想进行集成. 基分类器是CART. 
* 有放回地抽取样本, 进行训练, 得到一棵树. 然后很多这样的树构成森林. 每一棵树在构建的时候, 只选取部分的特征来进行分裂. 
* 优点: 
    * 无需降维; 
    * 能够判断特征的重要程度; 
    * 特征之间的影响也能判断; 
    * 不易过拟合; 
    * 训练快, 能并行; 
    * 实现较为简单; 
    * *对于不平衡的数据集来说，它可以平衡误差;* 就算有很多的特征遗失, 也能维持准确度.  
* 缺点:     
    * 某些噪声较大的分类/回归问题上, 还是容易过拟合; 
    * *对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的.*

## GBDT

[参考1](https://puluwen.github.io/2019/01/GBDT-introduction/), [参考2](https://www.jianshu.com/p/856ea8201f63)

* 最基本的含义: *利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中的残差的近似值，拟合一个回归树.* 最基本的思想就是, 用后来的模型去拟合(或者形象一点, 弥补)前面模型预测结果与真实值之间的残差. 
* 然后呢, 这里面的梯度指什么呢? 是用某个梯度的值来作为这个所谓的残差.  
* 优点: 
    * 防止过拟合; 
    * 非线性变化多, 表达能力强, 无需太复杂的特征工程和变换; 
    * 可解释性强; 
* 缺点: 
    * 不利于串行化. 

## XGBoost

[参考1](https://zhuanlan.zhihu.com/p/92837676)

本质上和GBDT一脉相承, 但是这种算法有了更多的优化. 

详见下一题, 看看GBDT和XGBoost的对比. 

## RF, GBDT, XGBoost的优缺点

[参考1](https://www.jianshu.com/p/7c621813728c), [参考2](https://zhuanlan.zhihu.com/p/72247243), [参考3](https://mp.weixin.qq.com/s?__biz=MzI1MzY0MzE4Mg==&mid=2247485159&idx=1&sn=d429aac8370ca5127e1e786995d4e8ec&chksm=e9d01626dea79f30043ab80652c4a859760c1ebc0d602e58e13490bf525ad7608a9610495b3d&scene=21#wechat_redirect)

* RF Vs. GBDT
    * 集成学习：RF属于bagging思想，而GBDT是boosting思想
    * 前者减少方差variance, 后者减少偏差bias. 
    * 训练样本：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本
    * 随机森林可以用回归树和分类树, 但是GBDT只能用回归树. 
    * 前者可串行, 后者不行. 
    * 前者最后的结果是投票选出来的; 后者是将所有的结果加权累加得到的. 
    * 前者对异常值比较不敏感; 后者敏感. 
    * *随机森林对训练集一视同仁，GBDT 是基于权值的弱分类器的集成。* 我个人认为, 前半句指的是随机森林对每一个基分类器没有区别对待. 
    * 泛化能力：RF不易过拟合，而GBDT容易过拟合

* GBDT Vs. XGBoost: 
    * 前者: 用梯度下降法进行优化; 后者是牛顿法. 
    * 前者用了一阶梯度, 后者用了二阶梯度. (应该是因为后者用了二阶的泰勒展开)
    * 后者有正则项, 缺失值处理的方案, 支持并行(特征粒度上的并行, 而非是树粒度上的并行). 前者都没有. 
    * 后者借鉴了RF里面的特征抽样以防止过拟合以及减少计算量. 
    * 前者使用CART回归树为基学习器, 后者不仅可以用这个, 还能用线性的分类器(这样一来就相当于带正则项的逻辑回归或者线性回归).
    * 后者的Shrinkage参数相当于学习率. (这一点我不是很清楚到底是啥意思)

## 为什么RF能够降低Variance? 

[参考1](https://www.zhihu.com/question/26760839) 第一个回答, 我觉得说的还行. 简单来说, 就是randomforest的每一个基础模型每一个的bias, variance都差不多, 所以凑在一起不会降低bias (E(sigma Xi/n) = E(Xi)), 但却会降低variance (var(sigma Xi/n) = var(Xi)/n)

## 改变随机森林的训练样本数据量，是否会影响到随机森林学习到的模型的复杂度

[参考1](https://www.cnblogs.com/lestatzhang/p/10611332.html#_77)

看样子是会的. *默认参数下模型复杂度是：O(MNlog(N)) ， 其中 M 是树的数目， N 是样本数。*
