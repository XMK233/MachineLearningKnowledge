# 使用说明

题目之来源: <https://www.nowcoder.com/discuss/33737>

斜体字的是直接抄上面网页的. 

引用来源已以最大限度标注. 

此文乃徒自益尔, 言语甚鄙, 不求甚解, 万望阅者谅解. 如有谬误, 敬请指正. 

## ID3, C4.5, CART

* ID3是信息增益, 偏向于选取取值多的特征来划分枝干. 
* C4.5是用信息增益率. 就是在信息增益的基础上, 除以了一个反映特征本身特性的东西(好像叫固有值intrinsic value), 所以不会偏向于取值多的特征, 但是好像会偏向于取值少的特征. 
* CART用的是基尼系数. *回归树用平方误差最小化准则，分类树用基尼指数最小化准则.* 基尼系数能够表示一堆数据里面任抽俩, 它俩不同分类的概率是多少. 基尼系数反映了一个样本集合的纯度, 基尼系数越小越纯. 

可参考<https://www.jianshu.com/p/9b35f9c7f6fd> 其总结甚善. 可以了解到各自的优缺点. 

## 信息增益的公式

(咕咕咕)

## 决策树处理连续值的方法

* ID3没有办法应对连续值. 
* C4.5和CART能够应对连续值. 简单来说, 就是把所有的特征值排序, 然后一个一个地划分进`大于`, `小于`这两个类别里面, 然后计算增益率或者基尼系数, 选取最佳的划分点. [参考](https://www.jianshu.com/p/9b35f9c7f6fd). 

## 决策树防止过拟合

* 早停: 达到最大深度, 继续分类也得不到可观的分类误差缩小, 叶子节点数据量过小等.  [参考](https://blog.csdn.net/u014303046/article/details/53453177)
* 预剪枝: *预剪枝就是在构造决策树的过程中，先对每个结点在划分前进行估计，若果当前结点的划分不能带来决策树模型泛华性能的提升，则不对当前结点进行划分并且将当前结点标记为叶结点。* [参考](https://blog.csdn.net/u012328159/article/details/79285214)
* 后剪枝: 后剪枝就是先把整颗决策树构造完毕，然后自底向上的对非叶结点进行考察，若将该结点对应的子树换为叶结点能够带来泛化性能的提升，则把该子树替换为叶结点。[参考](https://blog.csdn.net/u012328159/article/details/79285214)

## 决策树怎么应对缺失值: 

* 决策树本身的算法是可以考虑缺失值的. 
    * 缺失值本身就是一个分支. 
    * 概率权重: C4.5采用这种方法. 每一个样本都有一个权值. 计算信息增益的式子也变了, 会考虑到样本的权值. 若某样本的属性a的取值未知, 那么这个样本会被划入所有的子节点, 然后权值会变小. *直观来看, 这就是让同一个样本以不同的概率划入到不同的子节点中去.* 参考: 西瓜书
* 对数据集做一些工作: 
    * 删掉有缺失值的行. 当然, 如果这样的行很多, 或者数据很宝贵, 那最好不要酱做. [参考](https://blog.csdn.net/u012328159/article/details/79413610)
    * 对数据进行填补. 比如用一些什么插值, 以某种概率分布来生成一个填补值. [参考](https://www.jianshu.com/p/2abc638490e3)
